{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 包导入\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "import numpy as np\n",
    "\n",
    "import mne\n",
    "import matplotlib.pyplot as pl\n",
    "%matplotlib inline\n",
    "# from functions import ReverseLayerF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型函数声明\n",
    "\n",
    "1. `__init__(classes 表示目标分类的类别数量。\n",
    "channels 表示输入数据的通道数。\n",
    "F1 表示卷积核的数量。\n",
    "D 表示深度可分离卷积的倍数。\n",
    "domains 表示源领域的数量。）`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型定义\n",
    "# coding=utf-8\n",
    "\n",
    "class DG_Network(nn.Module):\n",
    "    def __init__(self, classes, channels, F1=4, D=2, domains=3): \n",
    "        super(DG_Network, self).__init__()\n",
    "        self.dropout = 0.25  # default:0.25\n",
    "\n",
    "        # 四个并行的卷积块，用于提取不同尺度的特征。\n",
    "        self.block1_1 = nn.Sequential(  \n",
    "            nn.ZeroPad2d((3, 4, 0, 0)),\n",
    "            nn.Conv2d(1, F1, kernel_size=(1, 8), bias=False),  \n",
    "            nn.BatchNorm2d(F1)\n",
    "        )\n",
    "\n",
    "        self.block1_2 = nn.Sequential(  \n",
    "            nn.ZeroPad2d((7, 8, 0, 0)),\n",
    "            nn.Conv2d(1, F1, kernel_size=(1, 16), bias=False), \n",
    "            nn.BatchNorm2d(F1)\n",
    "        )\n",
    "\n",
    "        self.block1_3 = nn.Sequential(  \n",
    "            nn.ZeroPad2d((15, 16, 0, 0)),\n",
    "            nn.Conv2d(1, F1, kernel_size=(1, 32), bias=False), \n",
    "            nn.BatchNorm2d(F1)\n",
    "        )\n",
    "\n",
    "        self.block1_4 = nn.Sequential( \n",
    "            nn.ZeroPad2d((31, 32, 0, 0)),\n",
    "            nn.Conv2d(1, F1, kernel_size=(1, 64), bias=False), \n",
    "            nn.BatchNorm2d(F1)\n",
    "        )\n",
    "\n",
    "        # 深度可分离卷积块\n",
    "        self.block2 = nn.Sequential(  \n",
    "            # DepthwiseConv2D\n",
    "            nn.Conv2d(F1 * 4, F1 * 4 * D, kernel_size=(channels, 1), groups=F1 * 4, bias=False),\n",
    "            # groups=F1 for depthWiseConv \n",
    "            nn.BatchNorm2d(F1 * 4 * D),\n",
    "            # nn.ELU(inplace=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d((1, 4)),  \n",
    "            nn.Dropout(self.dropout),\n",
    "        )\n",
    "\n",
    "        # 四个并行的深度可分离卷积块，用于更加复杂的特征学习。\n",
    "        self.block3_1 = nn.Sequential(  \n",
    "            # SeparableConv2D\n",
    "            nn.ZeroPad2d((0, 1, 0, 0)),\n",
    "            nn.Conv2d(F1 * 4 * D, F1 * 4 * D, kernel_size=(1, 2), groups=F1 * 4 * D, bias=False),\n",
    "            # groups=F1 for depthWiseConv \n",
    "            nn.BatchNorm2d(F1 * 4 * D),\n",
    "            # nn.ELU(inplace=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(F1 * 4 * D, F1 * 4 * D, kernel_size=(1, 1), groups=1, bias=False),  # point-wise cnn  \n",
    "            nn.BatchNorm2d(F1 * 4 * D),\n",
    "        )\n",
    "\n",
    "        self.block3_2 = nn.Sequential(\n",
    "            # SeparableConv2D\n",
    "            nn.ZeroPad2d((1, 2, 0, 0)),\n",
    "            nn.Conv2d(F1 * 4 * D, F1 * 4 * D, kernel_size=(1, 4), groups=F1 * 4 * D, bias=False),\n",
    "            # groups=F1 for depthWiseConv  \n",
    "            nn.BatchNorm2d(F1 * 4 * D),\n",
    "            # nn.ELU(inplace=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(F1 * 4 * D, F1 * 4 * D, kernel_size=(1, 1), groups=1, bias=False),  # point-wise cnn   \n",
    "            nn.BatchNorm2d(F1 * 4 * D),\n",
    "        )\n",
    "\n",
    "        self.block3_3 = nn.Sequential(\n",
    "            # SeparableConv2D\n",
    "            nn.ZeroPad2d((3, 4, 0, 0)),\n",
    "            nn.Conv2d(F1 * 4 * D, F1 * 4 * D, kernel_size=(1, 8), groups=F1 * 4 * D, bias=False),\n",
    "            # groups=F1 for depthWiseConv \n",
    "            nn.BatchNorm2d(F1 * 4 * D),\n",
    "            # nn.ELU(inplace=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(F1 * 4 * D, F1 * 4 * D, kernel_size=(1, 1), groups=1, bias=False),  # point-wise cnn \n",
    "            nn.BatchNorm2d(F1 * 4 * D),\n",
    "        )\n",
    "\n",
    "        self.block3_4 = nn.Sequential(\n",
    "            # SeparableConv2D\n",
    "            nn.ZeroPad2d((7, 8, 0, 0)),\n",
    "            nn.Conv2d(F1 * 4 * D, F1 * 4 * D, kernel_size=(1, 16), groups=F1 * 4 * D, bias=False),\n",
    "            # groups=F1 for depthWiseConv \n",
    "            nn.BatchNorm2d(F1 * 4 * D),\n",
    "            # nn.ELU(inplace=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(F1 * 4 * D, F1 * 4 * D, kernel_size=(1, 1), groups=1, bias=False),  # point-wise cnn  \n",
    "            nn.BatchNorm2d(F1 * 4 * D),\n",
    "        )\n",
    "\n",
    "        # 全局平均池化和 dropout 的块。\n",
    "        self.block4 = nn.Sequential(\n",
    "            # nn.ELU(inplace=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d((1, 8)), \n",
    "            nn.Dropout(self.dropout)\n",
    "        )\n",
    "\n",
    "        # 三个全连接层，用于生成特定的特征。\n",
    "        self.special_features1 = nn.Sequential( \n",
    "            nn.Linear(3968, 400),\n",
    "            # nn.Dropout(self.dropout)\n",
    "        )\n",
    "\n",
    "        self.special_features2 = nn.Sequential( \n",
    "            nn.Linear(3968, 400),\n",
    "            # nn.Dropout(self.dropout)\n",
    "        )\n",
    "\n",
    "        self.special_features3 = nn.Sequential(  \n",
    "            nn.Linear(3968, 400),\n",
    "            # nn.Dropout(self.dropout)\n",
    "        )\n",
    "\n",
    "        # 领域分类器，用于领域分类的全连接层\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(3968, domains),\n",
    "        )\n",
    "\n",
    "        # 最终分类器，进行目标分类\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(400, classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, data_train1, data_train2, data_train3): \n",
    "        data1 = data_train1.to(torch.float32)\n",
    "        data2 = data_train2.to(torch.float32)\n",
    "        data3 = data_train3.to(torch.float32)\n",
    "        data = torch.cat((data1, data2, data3), dim=0)\n",
    "\n",
    "        # extracting general features\n",
    "        feat_1 = self.block1_1(data)   \n",
    "        feat_2 = self.block1_2(data)\n",
    "        feat_3 = self.block1_3(data)\n",
    "        feat_4 = self.block1_4(data)\n",
    "        feat = torch.cat((feat_1, feat_2, feat_3, feat_4), dim=1)  \n",
    "\n",
    "        # extracting special features\n",
    "        feature = self.block2(feat)              \n",
    "\n",
    "        feature_1 = self.block3_1(feature)\n",
    "        feature_2 = self.block3_2(feature)\n",
    "        feature_3 = self.block3_3(feature)\n",
    "        feature_4 = self.block3_4(feature)\n",
    "        features = torch.cat((feature_1, feature_2, feature_3, feature_4), dim=1) \n",
    "\n",
    "        features = self.block4(features)       \n",
    "\n",
    "        features = torch.flatten(features, 1)    \n",
    "\n",
    "\n",
    "        # extracting special features\n",
    "        feat1 = self.special_features1(features)  \n",
    "        feat2 = self.special_features2(features)\n",
    "        feat3 = self.special_features3(features)\n",
    "        Feat_s = [feat1, feat2, feat3]\n",
    "\n",
    "        # feat for domain classifier, dom for computing domain specific loss\n",
    "        feat_ = self.domain_classifier(features)\n",
    "        weight = nn.functional.softmax(feat_, dim=1)\n",
    "\n",
    "        feat123 = torch.stack((feat1, feat2, feat3), dim=1)   \n",
    "        weighted = weight.unsqueeze(0).permute(1, 0, 2)\n",
    "        weighted_feature = torch.bmm(weighted, feat123)      \n",
    "        weighted_feature = torch.flatten(weighted_feature, 1)\n",
    "        feature = self.classifier(weighted_feature)\n",
    "        out = nn.functional.softmax(feature, dim=1)\n",
    "\n",
    "        return out, weight, Feat_s, weighted_feature   #\n",
    "\n",
    "\n",
    "    def predict(self, data):\n",
    "        data = data.to(torch.float32)\n",
    "\n",
    "        feat_1 = self.block1_1(data)  \n",
    "        feat_2 = self.block1_2(data)\n",
    "        feat_3 = self.block1_3(data)\n",
    "        feat_4 = self.block1_4(data)\n",
    "        feat = torch.cat((feat_1, feat_2, feat_3, feat_4), dim=1)\n",
    "\n",
    "        # extracting special features\n",
    "        feature = self.block2(feat)          \n",
    "        feature_1 = self.block3_1(feature)\n",
    "        feature_2 = self.block3_2(feature)\n",
    "        feature_3 = self.block3_3(feature)\n",
    "        feature_4 = self.block3_4(feature)\n",
    "\n",
    "        features = torch.cat((feature_1, feature_2, feature_3, feature_4), dim=1) \n",
    "        features = self.block4(features)      \n",
    "        features = torch.flatten(features, 1) \n",
    "\n",
    "        # extracting special features\n",
    "        feat1 = self.special_features1(features)  \n",
    "        feat2 = self.special_features2(features)\n",
    "        feat3 = self.special_features3(features)\n",
    "\n",
    "        # feat for domain classifier, dom for computing domain specific loss\n",
    "        feat_ = self.domain_classifier(features)\n",
    "        weight = nn.functional.softmax(feat_, dim=1)\n",
    "\n",
    "        feat123 = torch.stack((feat1, feat2, feat3), dim=1) \n",
    "        weighted = weight.unsqueeze(0).permute(1, 0, 2)\n",
    "        weighted_feature = torch.bmm(weighted, feat123)  \n",
    "        weighted_feature = torch.flatten(weighted_feature, 1)\n",
    "        feature = self.classifier(weighted_feature)\n",
    "        out = nn.functional.softmax(feature, dim=1)\n",
    "\n",
    "        return out, weight, weighted_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "dataDir = \"./DataProcessed/BCI2a/data/\"\n",
    "labelDir = \"./DataProcessed/BCI2a/labels/\"\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, data_path, labels_path):\n",
    "        self.data = np.load(data_path)\n",
    "        self.labels = np.load(labels_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            'data': torch.tensor(self.data[idx], dtype=torch.float32),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "# 假设你有三组数据集 data_A01T.npy, labels_A01T.npy，以及一个测试集 data_test.npy\n",
    "# 分别创建对应的 Dataset 实例\n",
    "dataset1 = EEGDataset(dataDir+'data_A01T.npy', labelDir+'labels_A01T.npy')\n",
    "dataset2 = EEGDataset(dataDir+'data_A02T.npy', labelDir+'labels_A02T.npy')\n",
    "dataset3 = EEGDataset(dataDir+'data_A03T.npy', labelDir+'labels_A03T.npy')\n",
    "test_dataset = EEGDataset(dataDir+'data_A05T.npy', labelDir+'labels_A05T.npy')\n",
    "\n",
    "# 创建对应的 DataLoader 实例\n",
    "dataloader1 = DataLoader(dataset1, batch_size=8, shuffle=True)\n",
    "dataloader2 = DataLoader(dataset2, batch_size=8, shuffle=True)\n",
    "dataloader3 = DataLoader(dataset3, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练\n",
    "\n",
    "先拿3个数据 + 1个验证集练练手"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learning_rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Codeprojects\\GraduateProject\\train.ipynb 单元格 5\u001b[0m line \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Codeprojects/GraduateProject/train.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# 定义损失函数和优化器\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Codeprojects/GraduateProject/train.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Codeprojects/GraduateProject/train.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Codeprojects/GraduateProject/train.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# 数据集准备\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Codeprojects/GraduateProject/train.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# train_loader1, train_loader2, train_loader3 是三组数据的 DataLoader\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Codeprojects/GraduateProject/train.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Codeprojects/GraduateProject/train.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# 训练循环\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Codeprojects/GraduateProject/train.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'learning_rate' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "num_classes = 4\n",
    "learning_rate = 0.005\n",
    "\n",
    "# 初始化模型\n",
    "model = DG_Network(classes=num_classes, channels=22)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 500\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_data1, batch_data2, batch_data3 in zip(dataloader1, dataloader2, dataloader3):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 提取数据和标签\n",
    "        data1, labels1 = batch_data1['data'], batch_data1['label']\n",
    "        data2, labels2 = batch_data2['data'], batch_data2['label']\n",
    "        data3, labels3 = batch_data3['data'], batch_data3['label']\n",
    "\n",
    "        # 前向传播\n",
    "        outputs, _, _, _ = model(data1, data2, data3)\n",
    "\n",
    "        # 根据你的标签格式，可能需要修改损失函数的计算方式\n",
    "        # 这里假设你的标签是 one-hot 编码的，需要使用交叉熵损失\n",
    "        loss = criterion(outputs, torch.cat([labels1, labels2, labels3]))\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # 在每个周期结束时记录准确度等信息\n",
    "    # 可以根据需要添加其他的记录和验证步骤\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "\n",
    "# 训练完成后，你可以保存训练好的模型参数\n",
    "torch.save(model.state_dict(), './model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gpro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
